{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "f = open('./stopwords/StopWords_Auditor.txt', \"r\")\n",
    "data = f.read()\n",
    "l1 = data.split('\\n')\n",
    "print(len(l1))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    }
   ],
   "source": [
    "f = open('./stopwords/StopWords_Currencies.txt', \"r\")\n",
    "data = f.read()\n",
    "l2 = data.split('\\n')\n",
    "for i in range(0, len(l2)-1, 1):\n",
    "    l2[i] = l2[i].split()[0]\n",
    "print(len(l2))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    }
   ],
   "source": [
    "f = open('./stopwords/StopWords_DatesandNumbers.txt', \"r\")\n",
    "data = f.read()\n",
    "l3 = data.split('\\n')\n",
    "print(len(l3))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "f = open('./stopwords/StopWords_Generic.txt', \"r\")\n",
    "data = f.read()\n",
    "l4 = data.split('\\n')\n",
    "print(len(l4))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n"
     ]
    }
   ],
   "source": [
    "f = open('./stopwords/StopWords_GenericLong.txt', \"r\")\n",
    "data = f.read()\n",
    "l5 = data.split('\\n')\n",
    "print(len(l5))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "f = open('./stopwords/StopWords_Geographic.txt', \"r\")\n",
    "data = f.read()\n",
    "l6 = data.split('\\n')\n",
    "print(len(l6))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13015\n"
     ]
    }
   ],
   "source": [
    "f = open('./stopwords/StopWords_Names.txt', \"r\")\n",
    "data = f.read()\n",
    "l7 = data.split('\\n')\n",
    "print(len(l7))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8 = ['.', '/', ',', '?', ':', ';', \"'\", '\"', '(', ')', '%', '^', '&']\n",
    "l8 += ['[', ']', '{', '}', '|', '-', '_', '!', '@', '#', '$', '*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14136\n"
     ]
    }
   ],
   "source": [
    "stopwords = l1+l2+l3+l4+l5+l6+l7+l8\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/bhavya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/bhavya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from textstat.textstat import textstatistics\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./dict/negative-words.txt', 'r')\n",
    "masterneg = f.read()\n",
    "masterneg = masterneg.split('\\n')\n",
    "\n",
    "f = open('./dict/positive-words.txt', 'r')\n",
    "masterpos = f.read()\n",
    "masterpos = masterpos.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = ['I', 'We', 'we', 'my', 'My', 'ours', 'Ours', 'us', 'Us']\n",
    "\n",
    "directory = './files'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Analysis, Complex Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "df = pd.read_csv('./output.csv')\n",
    "for fname in os.listdir(directory):\n",
    "\n",
    "    f = open(os.path.join(directory, fname), 'r')\n",
    "    data = f.read()\n",
    "\n",
    "    tokenzz = word_tokenize(str(data))\n",
    "    tokenzz = [token for token in tokenzz if not token in stopwords]\n",
    "    newdata = [token for token in tokenzz if not token in stopwords]\n",
    "\n",
    "    negativedict = []\n",
    "    positivedict = []\n",
    "\n",
    "    complexwords = []\n",
    "\n",
    "    score_positive = 0\n",
    "    score_negative = 0\n",
    "    polarity = 0\n",
    "    subjectivity = 0\n",
    "\n",
    "    wc = 0\n",
    "    syllables = 0\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for word in newdata:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word in masterneg:\n",
    "            negativedict.append(word)\n",
    "            score_negative += 1\n",
    "        elif word in masterpos:\n",
    "            positivedict.append(word)\n",
    "            score_positive += 1\n",
    "\n",
    "    for word in tokenzz:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        syllables += textstatistics().syllable_count(word)\n",
    "        if textstatistics().syllable_count(word) > 2:\n",
    "            complexwords.append(word)\n",
    "        wc += 1\n",
    "    \n",
    "    polarity = (score_positive - score_negative)/(score_positive+score_negative+0.000001)\n",
    "    subjectivity = (score_positive+score_negative)/(len(newdata)+0.000001)\n",
    "\n",
    "    sentences = data.split('.')\n",
    "    words = data.split(' ')\n",
    "\n",
    "    senlen = []\n",
    "    senlenf = 0\n",
    "    for sentence in sentences:\n",
    "        senlen.append(len(sentence.split(' ')))\n",
    "    \n",
    "    for i in senlen:\n",
    "        senlenf += i\n",
    "\n",
    "    words = [word for word in words if not word in l8]\n",
    "\n",
    "    avgsenlen = len(words)/len(sentences)\n",
    "\n",
    "    complexpercent = len(complexwords)/len(newdata)\n",
    "    fog_index = 0.4*(avgsenlen + complexpercent)\n",
    "\n",
    "    complex_wc = len(complexwords)\n",
    "\n",
    "    syllable_count = syllables/len(newdata)\n",
    "\n",
    "    personal_pronouns = 0\n",
    "    word_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in pronouns:\n",
    "            personal_pronouns += 1\n",
    "        \n",
    "        word_length += len(word)\n",
    "\n",
    "    avgwordlen = word_length/len(words)\n",
    "\n",
    "    #print(score_positive, score_negative, polarity, subjectivity, avgsenlen, complexpercent, fog_index, avgsenlen, complex_wc, wc, syllable_count, personal_pronouns, avgwordlen)      \n",
    "    df.loc[idx, 'POSITIVE SCORE'] = score_positive\n",
    "    df.loc[idx, 'NEGATIVE SCORE'] = score_negative\n",
    "    df.loc[idx, 'POLARITY SCORE'] = polarity\n",
    "    df.loc[idx, 'SUBJECTIVITY SCORE'] = subjectivity\n",
    "    df.loc[idx, 'AVG SENTENCE LENGTH'] = avgsenlen\n",
    "    df.loc[idx, 'PERCENTAGE OF COMPLEX WORDS'] = complexpercent\n",
    "    df.loc[idx, 'FOG INDEX'] = fog_index\n",
    "    df.loc[idx, 'AVG NUMBER OF WORDS PER SENTENCE'] = avgsenlen\n",
    "    df.loc[idx, 'COMPLEX WORD COUNT'] = complex_wc\n",
    "    df.loc[idx, 'WORD COUNT'] = wc\n",
    "    df.loc[idx, 'SYLLABLE PER WORD'] = syllable_count\n",
    "    df.loc[idx, 'PERSONAL PRONOUNS'] = personal_pronouns\n",
    "    df.loc[idx, 'AVG WORD LENGTH'] = avgwordlen\n",
    "    df.to_csv('./output.csv', index=False)\n",
    "\n",
    "    idx += 1   \n",
    "    print (idx)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
